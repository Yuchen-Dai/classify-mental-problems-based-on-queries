{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "544 Psych_Queries_Classification ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Model:\n",
        "Embedding: **Senstence BERT** (Ref:https://www.sbert.net/)\n",
        "\n",
        "Processing Model: **Multilayer Perceptron(MLP)** from scikit-learn\n",
        "\n",
        "Accuracy: Over 80\n",
        "\n",
        "Run code thru colab:https://colab.research.google.com/drive/1LZcNvFpkZTdeAbpy8FHvGUHG1GKV0Xna#scrollTo=RAyehqJ5bDpd"
      ],
      "metadata": {
        "id": "dU0Ki4vfWahk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "collapsed": true,
        "id": "FjjuvSA7WuN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874b1b6f-b383-4511-969e-40b783c69131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "display.max_colwidth : int or None\n",
            "    The maximum width in characters of a column in the repr of\n",
            "    a pandas data structure. When the column overflows, a \"...\"\n",
            "    placeholder is embedded in the output. A 'None' value means unlimited.\n",
            "    [default: 50] [currently: 200]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('max_rows', 100)\n",
        "pd.set_option('max_colwidth', 200)\n",
        "pd.describe_option('max_colwidth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/leadlost/classify-mental-problems-based-on-queries/main/new_dataset.csv')\n",
        "dataset = pd.DataFrame(dataset, columns = ['idx', 'labels','content'])\n",
        "dataset.pop('idx')\n",
        "\n",
        "# Seperate labels\n",
        "res_list = [] #Array of labels with type int\n",
        "labels = []\n",
        "count = 0\n",
        "class_names = []\n",
        "for i in range(11):\n",
        "  name = []\n",
        "  class_names.append(name)\n",
        "\n",
        "for row in dataset['labels'].values.tolist():\n",
        "  label = row.replace(',','')\n",
        "  label = label.replace('[','')\n",
        "  label = label.replace(']','')\n",
        "  label = label.split() #Split to List\n",
        "  row_res = []\n",
        "  for ele in label:\n",
        "    ele = int(ele)\n",
        "    row_res.append(ele) \n",
        "  row_res = pd.Series(row_res)\n",
        "  res_list.append(row_res)\n",
        " \n",
        "  for i in range(len(label)):\n",
        "    val = label[i]\n",
        "    val = int(val)\n",
        "    class_names[i].append(val)\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "  name = 'class'+str(i)\n",
        "  dataset[name] = class_names[i]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wGykd1iGW7tw"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.columns)\n",
        "print(type(res_list))\n",
        "dataset['res_list'] = res_list\n",
        "print(dataset.columns)"
      ],
      "metadata": {
        "id": "0qOFDUIsLlKT",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff025d3-a19f-48f7-bb35-ab7762058bc8"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['labels', 'content', 'class0', 'class1', 'class2', 'class3', 'class4',\n",
            "       'class5', 'class6', 'class7', 'class8', 'class9', 'class10'],\n",
            "      dtype='object')\n",
            "<class 'list'>\n",
            "Index(['labels', 'content', 'class0', 'class1', 'class2', 'class3', 'class4',\n",
            "       'class5', 'class6', 'class7', 'class8', 'class9', 'class10',\n",
            "       'res_list'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SBERT Embedding\n",
        "%%time\n",
        "!pip install -U sentence-transformers\n",
        "from numpy.ma.core import shape\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "embeddings = []\n",
        "for row in dataset['content'].values.tolist():\n",
        "  embedding = model.encode(row)\n",
        "  embeddings.append(embedding)\n",
        "dataset['embeddings'] = embeddings\n",
        "print(dataset.columns)"
      ],
      "metadata": {
        "id": "qHS4XcR5F0H-",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e01c63-6954-4702-f045-bbcc7ebed250"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Index(['labels', 'content', 'class0', 'class1', 'class2', 'class3', 'class4',\n",
            "       'class5', 'class6', 'class7', 'class8', 'class9', 'class10', 'res_list',\n",
            "       'embeddings'],\n",
            "      dtype='object')\n",
            "CPU times: user 50.1 s, sys: 440 ms, total: 50.6 s\n",
            "Wall time: 1min 6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report\n",
        "X = dataset['embeddings'].values.tolist()\n",
        "Y = dataset['class1']\n",
        "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.2, random_state = 1)#random_state = 1\n",
        "features = []\n",
        "print(testY.index.values.tolist())\n",
        "for idx in testY.index.values.tolist():\n",
        "  features.append(res_list[idx])\n",
        "print(len(features))"
      ],
      "metadata": {
        "id": "RAyehqJ5bDpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff483aab-f6bd-41fd-b36d-0e298a0d7f8a"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[625, 318, 339, 598, 90, 666, 262, 84, 585, 85, 567, 664, 216, 189, 19, 273, 314, 375, 60, 550, 600, 59, 488, 432, 632, 414, 47, 388, 544, 738, 559, 181, 310, 457, 667, 285, 506, 740, 187, 104, 101, 663, 8, 331, 117, 69, 255, 493, 522, 486, 402, 529, 201, 265, 716, 81, 394, 378, 411, 434, 148, 56, 289, 502, 736, 479, 23, 195, 596, 683, 671, 656, 154, 651, 579, 323, 202, 725, 591, 50, 528, 626, 120, 495, 224, 428, 669, 175, 185, 159, 435, 61, 214, 540, 382, 597, 3, 622, 441, 134, 329, 65, 593, 180, 111, 602, 57, 543, 419, 355, 577, 358, 403, 268, 247, 35, 383, 710, 334, 527, 41, 551, 630, 16, 556, 108, 617, 747, 379, 0, 241, 74, 293, 66, 385, 301, 496, 696, 363, 691, 555, 257, 745, 421, 13, 452, 739, 519, 92, 472]\n",
            "150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP CLASSIFIER\n",
        "max_iter = 100\n",
        "mlp = MLPClassifier(    \n",
        "                    max_iter = max_iter,\n",
        "                    solver =  'adam',\n",
        "                    activation = 'relu', \n",
        "                    early_stopping= True, \n",
        "                    random_state = 5,\n",
        "                    hidden_layer_sizes = (150,)\n",
        "                    )\n",
        "# TRAIN\n",
        "mlp.fit(trainX, trainY)\n",
        "print(mlp.get_params())\n",
        "# Pred\n",
        "trainPred = mlp.predict(trainX)\n",
        "testPred = mlp.predict(testX)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# plt.plot(mlp.loss_curve_)\n",
        "# print('Training set accuracy', mlp.score(trainX, trainY))\n",
        "# print('Development set accuracy', mlp.score(testX, testY))\n",
        "# print(confusion_matrix(testY, testPred))\n",
        "# print(classification_report(testY, testPred)) # output_dict=True, makes the output a dict\n",
        "# print(accuracy_score(testY, testPred))\n",
        "# print(accuracy_score(testY, testPred, normalize = False))# num of the correct prediction\n",
        "print(testY)"
      ],
      "metadata": {
        "id": "P5SvCFHdOpjx",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5339e2ba-9506-413f-a2e8-77666415789c"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (150,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 100, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 5, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
            "625    0\n",
            "318    0\n",
            "339    0\n",
            "598    0\n",
            "90     0\n",
            "      ..\n",
            "452    0\n",
            "739    0\n",
            "519    0\n",
            "92     1\n",
            "472    0\n",
            "Name: class1, Length: 150, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_results = []\n",
        "accuracies = []\n",
        "testPreds = []\n",
        "for i in range(11):\n",
        "  X = dataset['embeddings'].values.tolist()\n",
        "  name = 'class'+str(i)\n",
        "  Y = dataset[name]\n",
        "  # Split train test\n",
        "  trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
        "  true_results.append(testY) \n",
        "  # TRAIN\n",
        "  mlp.fit(trainX, trainY)\n",
        "  # Predict\n",
        "  testPred = mlp.predict(testX)\n",
        "  testPreds.append(testPred.tolist())\n",
        "  # Evaluate\n",
        "  accuracy = accuracy_score(testY, testPred)\n",
        "  accuracies.append(accuracy)\n",
        "print(accuracies)\n",
        "print(len(true_results))"
      ],
      "metadata": {
        "id": "GPr3hgT6Rger",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa06d4d4-b950-48f4-bcd8-776e833eee18"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8666666666666667, 0.8533333333333334, 0.9066666666666666, 0.8533333333333334, 0.8, 0.9066666666666666, 0.9333333333333333, 0.96, 0.9466666666666667, 0.9, 0.9466666666666667]\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label: true_results\n",
        "# Predictions: df_testPreds\n",
        "df_testPreds = pd.DataFrame(testPreds)\n",
        "true_results = pd.DataFrame(true_results)\n",
        "df_testPreds = df_testPreds.values.flatten()\n",
        "true_results = true_results.values.flatten()\n",
        "print(classification_report(true_results, df_testPreds))\n",
        "from sklearn.metrics import f1_score\n",
        "print('macro avg: ',f1_score(true_results, df_testPreds, average='macro'))\n",
        "print('micro avg: ',f1_score(true_results, df_testPreds, average='micro'))\n",
        "print('weighted avg: ',f1_score(true_results, df_testPreds, average='weighted'))"
      ],
      "metadata": {
        "id": "0XWVG3cZ_2-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27f4dde-f880-4895-ea63-4db7c78c6c9c"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.94      1414\n",
            "           1       0.77      0.40      0.53       236\n",
            "\n",
            "    accuracy                           0.90      1650\n",
            "   macro avg       0.84      0.69      0.74      1650\n",
            "weighted avg       0.89      0.90      0.88      1650\n",
            "\n",
            "macro avg:  0.7358922315283207\n",
            "micro avg:  0.8975757575757576\n",
            "weighted avg:  0.8834237525847736\n"
          ]
        }
      ]
    }
  ]
}